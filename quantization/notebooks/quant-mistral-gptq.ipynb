{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Mistral 7B GPTQ Quantization and Evaluation","metadata":{"_uuid":"4c952489-07f2-4a6f-b725-f0c0964669c1","_cell_guid":"e367953a-89e3-431c-9f46-7746559f4bad","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"# Phase 1: Quantization (P100)","metadata":{"_uuid":"62f61028-3112-4d00-9a25-5ca0c96f9ce3","_cell_guid":"800ff783-7844-4092-a917-a04f6ae41aca","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install -q auto-gptq optimum transformers accelerate datasets torch huggingface_hub","metadata":{"_uuid":"2d73af9f-68e0-4806-92a2-dbeab88c26ac","_cell_guid":"580e9634-7144-4ede-9ecb-fd204e55c71e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport time\nimport torch\nimport json\nimport os\nfrom pathlib import Path\nfrom transformers import AutoTokenizer\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\nfrom datasets import load_dataset\nfrom huggingface_hub import login, HfApi\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\nmodel_id = \"mistralai/Mistral-7B-v0.1\"\nhf_username = \"zahraase1im\"\nbits = 4","metadata":{"_uuid":"1b4ac58b-3190-434c-bcd9-45b67620bbe4","_cell_guid":"471487cb-81e9-497b-984d-2309b532475b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clear_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndef prepare_calibration_data(tokenizer, n_samples=128, max_length=2048):\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")\n    dataset = dataset.filter(lambda x: len(x[\"text\"]) > 200)\n    \n    examples = []\n    for i in range(min(n_samples, len(dataset))):\n        text = dataset[i][\"text\"]\n        tokenized = tokenizer(text, return_tensors=\"pt\", max_length=max_length, \n                            truncation=True, padding=False)\n        examples.append({\n            \"input_ids\": tokenized[\"input_ids\"],\n            \"attention_mask\": tokenized[\"attention_mask\"]\n        })\n    return examples","metadata":{"_uuid":"21579e8d-e5d3-4cb1-9ecd-487aa31f1c2b","_cell_guid":"8fb32019-9522-47b4-a4a9-3add38dc00a0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nlogin(token=UserSecretsClient().get_secret(\"HF_TOKEN\"))","metadata":{"_uuid":"33793e67-7824-41e4-92c7-54a285552f7c","_cell_guid":"e0559de5-5443-4728-a092-5161094a08e8","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"_uuid":"48f74821-f9ba-4aea-a456-2c9dd51e8954","_cell_guid":"f8a358ca-c403-429d-a7eb-5123c3d21bd4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Preparing calibration data\")\ncalibration_data = prepare_calibration_data(tokenizer, n_samples=128, max_length=2048)\nprint(f\"Prepared {len(calibration_data)} samples\")","metadata":{"_uuid":"c6fa5253-a9b7-4124-8e28-f5fccbdb83cc","_cell_guid":"c68dcb04-2805-4fa6-9342-e89f4939a70e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Quantizing to {bits}-bit\")\nclear_memory()\n\nquantize_config = BaseQuantizeConfig(\n    bits=bits,\n    group_size=128,\n    desc_act=False,\n    damp_percent=0.01\n)\n\nmodel = AutoGPTQForCausalLM.from_pretrained(\n    model_id,\n    quantize_config=quantize_config,\n    max_memory={0: \"10GiB\", \"cpu\": \"50GiB\"}\n)\n\nquantize_start = time.time()\nmodel.quantize(calibration_data, use_triton=False, batch_size=1, cache_examples_on_gpu=False)\nquantize_time = time.time() - quantize_start\nprint(f\"Quantization completed in {quantize_time:.2f} seconds\")","metadata":{"_uuid":"2b869306-6b4e-4c54-813f-04ed57f4769f","_cell_guid":"493b669a-aeee-4310-8049-4aaea5a3cdc0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir = \"/kaggle/working/mistral-7b-gptq-4bit\"\nmodel.save_quantized(save_dir, use_safetensors=True)\ntokenizer.save_pretrained(save_dir)\n\nwith open(f\"{save_dir}/quantize_time.json\", \"w\") as f:\n    json.dump({\"quantization_time_seconds\": quantize_time}, f)\n\nprint(f\"Saved to {save_dir}\")","metadata":{"_uuid":"fa6fb573-39b9-4a56-ba9d-bb9d8aaaabd1","_cell_guid":"484e05a0-ccc7-4bd6-9620-44f83775296b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Uploading to HuggingFace: {hf_username}/mistral-7b-gptq-{bits}bit\")\napi = HfApi()\napi.create_repo(repo_id=f\"{hf_username}/mistral-7b-gptq-{bits}bit\", exist_ok=True, private=False)\napi.upload_folder(\n    folder_path=save_dir,\n    repo_id=f\"{hf_username}/mistral-7b-gptq-{bits}bit\",\n    repo_type=\"model\"\n)\nprint(\"Upload complete\")\n\ndel model\nclear_memory()","metadata":{"_uuid":"e0fcbf77-74a4-4838-961a-97c227a4c6de","_cell_guid":"66185adb-bda6-45be-aae9-d9779a892b33","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Phase 2: Evaluation with ExLlamaV2 (T4)","metadata":{"_uuid":"5d4749d4-2ae0-44ef-8f48-fc0b323443de","_cell_guid":"0fc438e6-948a-499f-88b6-97089e040864","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install -q exllamav2 transformers datasets huggingface_hub","metadata":{"_uuid":"5881f67d-f26d-457f-8c1c-3ad7155bdc93","_cell_guid":"1d6b2d12-7154-4ddd-bdde-46251c45b841","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nimport time\nimport torch\nimport json\nfrom pathlib import Path\nfrom datasets import load_dataset\nfrom exllamav2 import ExLlamaV2, ExLlamaV2Config, ExLlamaV2Cache, ExLlamaV2Tokenizer\nfrom exllamav2.generator import ExLlamaV2StreamingGenerator, ExLlamaV2Sampler","metadata":{"_uuid":"0bd9bd81-5060-4abe-b2bd-63112968ce46","_cell_guid":"6ea10a41-0604-4482-bc00-41acf50e5d10","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def clear_memory():\n    gc.collect()\n    torch.cuda.empty_cache()\n\ndef calculate_perplexity(model, tokenizer_obj, cache, max_samples=100, max_length=512):\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    dataset = dataset.filter(lambda x: len(x[\"text\"]) > 100)\n    \n    total_loss = 0\n    total_tokens = 0\n    \n    for i in range(min(max_samples, len(dataset))):\n        text = dataset[i][\"text\"]\n        cache.current_seq_len = 0\n        \n        input_ids = tokenizer_obj.encode(text)\n        if input_ids.shape[-1] > max_length:\n            input_ids = input_ids[:, :max_length]\n        if input_ids.shape[-1] < 2:\n            continue\n        \n        logits = model.forward(input_ids, cache, input_mask=None)\n        if logits.dim() == 3:\n            logits = logits.squeeze(0)\n        \n        shift_logits = logits[:-1, :]\n        shift_labels = input_ids[0, 1:].to(shift_logits.device)\n        \n        if shift_logits.shape[0] != shift_labels.shape[0]:\n            continue\n        \n        loss = torch.nn.functional.cross_entropy(\n            shift_logits,\n            shift_labels,\n            reduction='sum'\n        )\n        \n        total_loss += loss.item()\n        total_tokens += shift_labels.numel()\n    \n    return torch.exp(torch.tensor(total_loss / total_tokens)).item()\n\ndef test_generation(generator, prompt=\"The capital of France is\"):\n    settings = ExLlamaV2Sampler.Settings()\n    settings.temperature = 0.0\n    settings.top_k = 1\n    \n    start_time = time.time()\n    output = generator.generate_simple(prompt, settings, 50, seed=42)\n    latency = (time.time() - start_time) * 1000\n    \n    return output, latency\n\ndef get_model_size_from_files(model_path):\n    total_size = 0\n    model_dir = Path(model_path)\n    \n    for file in model_dir.glob(\"*.safetensors\"):\n        total_size += file.stat().st_size\n    for file in model_dir.glob(\"*.bin\"):\n        total_size += file.stat().st_size\n    \n    return total_size / (1024 ** 3)","metadata":{"_uuid":"7ae39857-bf47-4750-b864-693ff99fb034","_cell_guid":"0a963fa8-5cd9-4ce1-9d60-fd204f21b5e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-09T04:10:04.509994Z","iopub.execute_input":"2025-12-09T04:10:04.510722Z","iopub.status.idle":"2025-12-09T04:10:04.519612Z","shell.execute_reply.started":"2025-12-09T04:10:04.510696Z","shell.execute_reply":"2025-12-09T04:10:04.518864Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"model_path = \"/kaggle/working/mistral-7b-gptq-4bit\"\n\nconfig = ExLlamaV2Config()\nconfig.model_dir = model_path\nconfig.prepare()\n\nmodel = ExLlamaV2(config)\ncache = ExLlamaV2Cache(model, lazy=True)\nmodel.load_autosplit(cache)\n\ntokenizer_obj = ExLlamaV2Tokenizer(config)\ngenerator = ExLlamaV2StreamingGenerator(model, cache, tokenizer_obj)","metadata":{"_uuid":"4c9ad009-2d86-4b9a-83f3-6d9333addb1e","_cell_guid":"7bdb3a9f-d57c-4ffe-8a85-f4868a877796","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Calculating perplexity\")\nperplexity = calculate_perplexity(model, tokenizer_obj, cache, max_samples=100, max_length=512)","metadata":{"_uuid":"233fc91a-1d31-4523-8235-8b7c7460dbd3","_cell_guid":"6b7a7a70-f83b-4554-9938-f2e0abdae5f0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-09T04:10:09.264829Z","iopub.execute_input":"2025-12-09T04:10:09.265311Z","iopub.status.idle":"2025-12-09T04:10:39.698898Z","shell.execute_reply.started":"2025-12-09T04:10:09.265287Z","shell.execute_reply":"2025-12-09T04:10:39.698305Z"}},"outputs":[{"name":"stdout","text":"Calculating perplexity\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"print(\"Testing generation\")\nresponse, latency = test_generation(generator)","metadata":{"_uuid":"ea20d69a-bb73-4767-b7e6-239ff7ac8dd0","_cell_guid":"0cf1cc6d-2970-46ae-a6ee-ec2bc91c5354","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-09T04:10:39.699976Z","iopub.execute_input":"2025-12-09T04:10:39.700186Z","iopub.status.idle":"2025-12-09T04:10:40.679223Z","shell.execute_reply.started":"2025-12-09T04:10:39.700170Z","shell.execute_reply":"2025-12-09T04:10:40.678641Z"}},"outputs":[{"name":"stdout","text":"Testing generation\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"model_size_gb = get_model_size_from_files(model_path)","metadata":{"_uuid":"3ffc367b-b7e2-4182-a251-e0595afe6a6d","_cell_guid":"c1447eaf-7c19-4321-afdb-0a7a6c9cd24a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-09T04:10:40.679885Z","iopub.execute_input":"2025-12-09T04:10:40.680123Z","iopub.status.idle":"2025-12-09T04:10:40.685292Z","shell.execute_reply.started":"2025-12-09T04:10:40.680098Z","shell.execute_reply":"2025-12-09T04:10:40.684714Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"with open(f\"{model_path}/quantize_time.json\", \"r\") as f:\n    quantize_time_data = json.load(f)\n    quantize_time = quantize_time_data[\"quantization_time_seconds\"]\n\nresults = {\n    \"model\": \"mistral-7b-gptq-4bit\",\n    \"quantization_time_seconds\": quantize_time,\n    \"perplexity\": perplexity,\n    \"latency_ms\": latency,\n    \"model_size_gb\": model_size_gb,\n    \"example_prompt\": \"The capital of France is\",\n    \"example_response\": response\n}\n\nwith open(f\"{model_path}/results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(\"\\nResults:\")\nprint(json.dumps(results, indent=2))\n\nclear_memory()","metadata":{"_uuid":"a041ed38-9d6d-4c1e-8086-a0a0781e8d4c","_cell_guid":"f64f42dd-233c-4c16-b93a-61fb9d146a05","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-12-09T04:10:40.686572Z","iopub.execute_input":"2025-12-09T04:10:40.686810Z","iopub.status.idle":"2025-12-09T04:10:40.892297Z","shell.execute_reply.started":"2025-12-09T04:10:40.686795Z","shell.execute_reply":"2025-12-09T04:10:40.891675Z"}},"outputs":[{"name":"stdout","text":"\nResults:\n{\n  \"model\": \"mistral-7b-gptq-4bit\",\n  \"quantization_time_seconds\": 1272.1622755527496,\n  \"perplexity\": 4.959344863891602,\n  \"latency_ms\": 976.264476776123,\n  \"model_size_gb\": 3.8730560317635536,\n  \"example_prompt\": \"The capital of France is\",\n  \"example_response\": \"The capital of France is Paris.\\n\\n## What is the capital of France?\\n\\nParis\\n\\n## What is the capital of France and why?\\n\\nParis\\n\\n## What is the capital of France and why is it called Paris?\\n\\n\"\n}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}