{
  "experiment_info": {
    "base_model": "mistralai/Mistral-7B-v0.1",
    "focus": "RAG performance evaluation on 4-bit quantized models",
    "quantization_target": "4-bit (GPTQ, AWQ, or bitsandbytes)",
    "primary_metrics": [
      "reading_comprehension_accuracy",
      "attention_preservation",
      "context_retrieval_quality",
      "long_context_handling"
    ]
  },
  "common_args": {
    "model": "hf",
    "device": "cuda:0",
    "batch_size": 1,
    "dtype": "float16"
  },
  "quantization_configs": {
    "baseline_fp16": {
      "model_args": "pretrained={model_id},dtype=float16"
    },
    "4bit_nf4": {
      "model_args": "pretrained={model_id},load_in_4bit=True,bnb_4bit_quant_type=nf4,bnb_4bit_compute_dtype=float16"
    },
    "4bit_fp4": {
      "model_args": "pretrained={model_id},load_in_4bit=True,bnb_4bit_quant_type=fp4,bnb_4bit_compute_dtype=float16"
    },
    "gptq_4bit": {
      "model_args": "pretrained={model_id},gptq=True,dtype=float16"
    },
    "awq_4bit": {
      "model_args": "pretrained={model_id},awq=True,dtype=float16"
    }
  },
  "tasks": [
    {
      "name": "squad",
      "category": "rag_core",
      "num_fewshot": 0,
      "priority": "critical",
      "description": "Reading comprehension on Wikipedia - tests basic context understanding",
      "rag_aspects": ["context_retrieval", "exact_match"]
    },
    {
      "name": "squad_v2",
      "category": "rag_core",
      "num_fewshot": 0,
      "priority": "critical",
      "description": "SQuAD with unanswerable questions - tests robustness",
      "rag_aspects": ["context_retrieval", "answer_rejection"]
    },
    {
      "name": "nq_open",
      "category": "rag_core",
      "num_fewshot": 0,
      "priority": "critical",
      "description": "Natural questions from Google search - real-world RAG scenarios",
      "rag_aspects": ["open_domain_qa", "context_retrieval"]
    },
    {
      "name": "triviaqa",
      "category": "rag_core",
      "num_fewshot": 0,
      "priority": "critical",
      "description": "Trivia with evidence documents - multi-document reasoning",
      "rag_aspects": ["multi_document", "evidence_aggregation"]
    },
    {
      "name": "drop",
      "category": "rag_reasoning",
      "num_fewshot": 3,
      "priority": "high",
      "description": "Discrete reasoning over passages - numerical and logical reasoning",
      "rag_aspects": ["numerical_reasoning", "context_manipulation"]
    },
    {
      "name": "quac",
      "category": "rag_conversational",
      "num_fewshot": 0,
      "priority": "high",
      "description": "Conversational QA - dialogue context tracking",
      "rag_aspects": ["conversational_context", "coreference_resolution"]
    },
    {
      "name": "coqa",
      "category": "rag_conversational",
      "num_fewshot": 0,
      "priority": "high",
      "description": "Conversational question answering - multi-turn context",
      "rag_aspects": ["conversational_context", "context_carryover"]
    },
    {
      "name": "narrativeqa",
      "category": "rag_long_context",
      "num_fewshot": 0,
      "priority": "high",
      "description": "Long narrative understanding - tests long-range attention",
      "rag_aspects": ["long_context", "narrative_comprehension"]
    },
    {
      "name": "race",
      "category": "rag_reasoning",
      "num_fewshot": 0,
      "priority": "medium",
      "description": "Reading comprehension from exams - educational context",
      "rag_aspects": ["reading_comprehension", "educational_qa"]
    },
    {
      "name": "boolq",
      "category": "rag_verification",
      "num_fewshot": 0,
      "priority": "medium",
      "description": "Yes/no questions - passage verification",
      "rag_aspects": ["fact_verification", "binary_classification"]
    },
    {
      "name": "multirc",
      "category": "rag_reasoning",
      "num_fewshot": 0,
      "priority": "medium",
      "description": "Multi-sentence reasoning - complex context integration",
      "rag_aspects": ["multi_sentence_reasoning", "evidence_synthesis"]
    },
    {
      "name": "openbookqa",
      "category": "rag_knowledge",
      "num_fewshot": 0,
      "priority": "medium",
      "description": "Open book exam - external knowledge + context",
      "rag_aspects": ["knowledge_retrieval", "fact_integration"]
    },
    {
      "name": "hellaswag",
      "category": "reasoning",
      "num_fewshot": 0,
      "priority": "low",
      "description": "Commonsense reasoning - baseline cognitive ability",
      "rag_aspects": []
    },
    {
      "name": "winogrande",
      "category": "reasoning",
      "num_fewshot": 0,
      "priority": "low",
      "description": "Pronoun resolution - baseline language understanding",
      "rag_aspects": []
    },
    {
      "name": "piqa",
      "category": "reasoning",
      "num_fewshot": 0,
      "priority": "low",
      "description": "Physical commonsense - baseline reasoning",
      "rag_aspects": []
    },
    {
      "name": "arc_easy",
      "category": "reasoning",
      "num_fewshot": 0,
      "priority": "low",
      "description": "Science questions easy - baseline knowledge",
      "rag_aspects": []
    },
    {
      "name": "arc_challenge",
      "category": "reasoning",
      "num_fewshot": 0,
      "priority": "low",
      "description": "Science questions hard - baseline knowledge",
      "rag_aspects": []
    },
    {
      "name": "mmlu",
      "category": "knowledge",
      "num_fewshot": 5,
      "priority": "medium",
      "description": "Broad knowledge - baseline capability check",
      "rag_aspects": []
    },
    {
      "name": "gsm8k",
      "category": "math",
      "num_fewshot": 5,
      "priority": "low",
      "description": "Math reasoning - check numerical degradation",
      "rag_aspects": []
    },
    {
      "name": "truthfulqa_mc2",
      "category": "knowledge",
      "num_fewshot": 0,
      "priority": "medium",
      "description": "Truthfulness - hallucination check for RAG",
      "rag_aspects": ["factuality"]
    }
  ],
  "categories": {
    "rag_core": {
      "description": "Core RAG tasks - primary focus for quantization impact",
      "tasks": ["squad", "squad_v2", "nq_open", "triviaqa"],
      "weight": 1.0
    },
    "rag_reasoning": {
      "description": "RAG with complex reasoning over context",
      "tasks": ["drop", "race", "multirc"],
      "weight": 0.8
    },
    "rag_conversational": {
      "description": "Multi-turn conversational context tracking",
      "tasks": ["quac", "coqa"],
      "weight": 0.7
    },
    "rag_long_context": {
      "description": "Long-range context and attention preservation",
      "tasks": ["narrativeqa"],
      "weight": 0.9
    },
    "rag_verification": {
      "description": "Fact verification from passages",
      "tasks": ["boolq"],
      "weight": 0.6
    },
    "rag_knowledge": {
      "description": "Knowledge retrieval and integration",
      "tasks": ["openbookqa"],
      "weight": 0.6
    },
    "reasoning": {
      "description": "Baseline reasoning - not RAG-specific",
      "tasks": ["hellaswag", "winogrande", "piqa", "arc_easy", "arc_challenge"],
      "weight": 0.3
    },
    "knowledge": {
      "description": "Baseline knowledge - check general degradation",
      "tasks": ["mmlu", "truthfulqa_mc2"],
      "weight": 0.4
    },
    "math": {
      "description": "Mathematical reasoning - check numerical precision",
      "tasks": ["gsm8k"],
      "weight": 0.3
    }
  },
  "custom_rag_metrics": {
    "attention_preservation": {
      "description": "Measures if attention patterns to relevant context are preserved",
      "implementation": "custom_benchmark",
      "num_samples": 200,
      "num_documents": 5,
      "metrics": ["precision@1", "precision@3", "mean_rank", "mrr"]
    },
    "context_utilization": {
      "description": "Tests if model uses provided context vs. parametric knowledge",
      "implementation": "custom_benchmark",
      "num_samples": 100,
      "metrics": ["context_faithfulness", "answer_grounding"]
    },
    "retrieval_sensitivity": {
      "description": "Performance degradation with noisy/irrelevant documents",
      "implementation": "custom_benchmark",
      "noise_levels": [0, 25, 50, 75],
      "metrics": ["robustness_score", "signal_noise_ratio"]
    },
    "long_context_degradation": {
      "description": "Performance across different context lengths",
      "implementation": "custom_benchmark",
      "context_lengths": [512, 1024, 2048, 4096],
      "metrics": ["accuracy_by_length", "attention_decay"]
    }
  },
  "evaluation_protocol": {
    "order": [
      "rag_core",
      "rag_reasoning",
      "rag_conversational",
      "rag_long_context",
      "rag_verification",
      "rag_knowledge",
      "reasoning",
      "knowledge",
      "math"
    ],
    "early_stopping": {
      "enabled": true,
      "condition": "If rag_core average drops below 60% of baseline, flag for investigation"
    },
    "comparison_metrics": {
      "primary": ["accuracy", "f1", "exact_match"],
      "secondary": [
        "latency_ms_per_token",
        "memory_mb",
        "throughput_tokens_per_sec"
      ],
      "degradation_threshold": 0.05,
      "critical_threshold": 0.1
    }
  }
}
