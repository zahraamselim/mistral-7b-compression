\section{Methodology}

\subsection{Overview}

This study presents a comprehensive Retrieval-Augmented Generation (RAG) system optimized for deployment on consumer-grade hardware using 4-bit quantized Large Language Models. Our modular architecture addresses the compound resource constraints of edge deployment: concurrent allocation for vector indices, embedding models, dynamic Key-Value caches, and the generation model itself. All experiments are conducted on NVIDIA Tesla T4 (16GB VRAM) to simulate realistic edge deployment scenarios.

The RAG pipeline consists of seven sequential modules: (1) Ingestion for document extraction, (2) Processing for content structuring, (3) Chunking for optimal segmentation, (4) Embedding for vector representation, (5) Indexing for fast retrieval, (6) Retrieval for relevant context selection, and (7) Generation for answer synthesis. We evaluate the complete system under quantized model constraints, measuring both efficiency and faithfulness metrics.

\subsection{Hardware and Software Environment}

\textbf{Hardware Configuration:}
\begin{itemize}
    \item \textbf{Platform:} NVIDIA Tesla T4 GPU (16GB VRAM, Turing architecture)
    \item \textbf{TDP:} 70W (for energy consumption calculations)
    \item \textbf{System RAM:} 16GB DDR4
    \item \textbf{Storage:} NVMe SSD for index persistence
\end{itemize}

\textbf{Software Stack:}
\begin{itemize}
    \item \textbf{Framework:} PyTorch 2.x with CUDA 11.8
    \item \textbf{Quantization:} BitsAndBytes (NF4 4-bit), AutoAWQ
    \item \textbf{Embeddings:} sentence-transformers (BGE-small-en-v1.5)
    \item \textbf{Vector Store:} FAISS (IndexHNSWFlat)
    \item \textbf{LLM Inference:} llama-cpp-python (GGUF format)
    \item \textbf{Language Model:} Mistral-7B-Instruct-v0.2 (4-bit quantized)
\end{itemize}

\subsection{Base Models}

\subsubsection{Generation Model: Mistral-7B-Instruct-v0.2}

We select Mistral-7B-Instruct-v0.2 \cite{jiang2023mistral} for answer generation due to its strong instruction-following capabilities and efficient architecture:

\begin{itemize}
    \item \textbf{Parameters:} 7.24 billion
    \item \textbf{Architecture:} Decoder-only transformer with Grouped Query Attention (GQA, 8 KV heads) and Sliding Window Attention (4096-token window)
    \item \textbf{Context Window:} 32k tokens (using 4k for memory safety)
    \item \textbf{Quantization:} 4-bit NF4 via BitsAndBytes
    \item \textbf{Model Size:} 7.6 GB (quantized) vs. 14 GB (FP16)
    \item \textbf{Format:} GGUF Q4\_K\_M for llama-cpp-python inference
\end{itemize}

\textbf{Quantization Configuration:}
\begin{itemize}
    \item \textbf{Method:} 4-bit NormalFloat (NF4) following QLoRA protocol \cite{dettmers2023qlora}
    \item \textbf{Double Quantization:} Enabled (quantizes quantization constants using FP8)
    \item \textbf{Compute Dtype:} FP16 for dequantization operations
    \item \textbf{Block Size:} 64 (default BitsAndBytes configuration)
\end{itemize}

\textbf{Generation Parameters:}
\begin{itemize}
    \item \textbf{Max New Tokens:} 128 (concise answers)
    \item \textbf{Temperature:} 0.3 (low for factuality)
    \item \textbf{Top-p:} 0.9 (nucleus sampling)
    \item \textbf{Repetition Penalty:} 1.15
    \item \textbf{Do Sample:} False (deterministic decoding)
\end{itemize}

\subsubsection{Embedding Model: BGE-small-en-v1.5}

For dense semantic embeddings, we employ BAAI/bge-small-en-v1.5 \cite{xiao2023bge}, a compact sentence transformer optimized for retrieval:

\begin{itemize}
    \item \textbf{Dimensions:} 384 (compact yet expressive)
    \item \textbf{Model Size:} 133 MB on disk, ~300 MB VRAM when loaded
    \item \textbf{Max Sequence Length:} 512 tokens
    \item \textbf{Training:} 1B+ sentence pairs from diverse domains
    \item \textbf{Performance:} SBERT benchmark score: 68.06
    \item \textbf{Inference Speed:} 50ms/chunk on GPU, 200ms/chunk on CPU
    \item \textbf{Normalization:} L2-normalized outputs for cosine similarity via dot product
\end{itemize}

\textbf{Content-Type Prefixes:}

To improve retrieval accuracy, we apply type-specific prefixes \cite{xiao2023bge}:
\begin{itemize}
    \item Math equations: \texttt{"equation: \{content\}"}
    \item Code blocks: \texttt{"code: \{content\}"}
    \item Tables: \texttt{"table: \{content\}"}
    \item Plain text: \texttt{"passage: \{content\}"}
    \item Headings: \texttt{"title: \{content\}"}
\end{itemize}

\subsection{RAG Pipeline Architecture}

Our modular RAG system consists of seven sequential stages, each optimized for edge deployment constraints:

\begin{figure}[H]
\centering
\begin{tikzpicture}[
    node distance=0.8cm,
    block/.style={rectangle, draw, fill=blue!10, text width=2.5cm, text centered, minimum height=0.7cm, font=\small},
    arrow/.style={->, >=stealth, thick}
]

% Main pipeline
\node[block] (ingest) {Ingestion};
\node[block, below of=ingest] (process) {Processing};
\node[block, below of=process] (chunk) {Chunking};
\node[block, below of=chunk] (embed) {Embedding};
\node[block, below of=embed] (index) {Indexing};
\node[block, below of=index] (retrieve) {Retrieval};
\node[block, below of=retrieve] (generate) {Generation};

% Arrows
\draw[arrow] (ingest) -- (process);
\draw[arrow] (process) -- (chunk);
\draw[arrow] (chunk) -- (embed);
\draw[arrow] (embed) -- (index);
\draw[arrow] (index) -- (retrieve);
\draw[arrow] (retrieve) -- (generate);

% Labels
\node[right=0.5cm of ingest, font=\footnotesize, text width=3cm] {PDF $\rightarrow$ Structured Text};
\node[right=0.5cm of process, font=\footnotesize, text width=3cm] {Classification + Enrichment};
\node[right=0.5cm of chunk, font=\footnotesize, text width=3cm] {Semantic Segmentation};
\node[right=0.5cm of embed, font=\footnotesize, text width=3cm] {Dense + Sparse Vectors};
\node[right=0.5cm of index, font=\footnotesize, text width=3cm] {FAISS + BM25 + Graph};
\node[right=0.5cm of retrieve, font=\footnotesize, text width=3cm] {Hybrid Search + RRF};
\node[right=0.5cm of generate, font=\footnotesize, text width=3cm] {Quantized LLM};

\end{tikzpicture}
\caption{Modular RAG pipeline architecture for edge deployment}
\label{fig:rag_pipeline}
\end{figure}

\subsubsection{Module 1: Ingestion}

The ingestion module transforms raw PDF documents into structured, searchable content using intelligent routing to optimize for both speed and quality.

\textbf{Routing Strategy:}

We employ a hybrid extraction approach that analyzes PDF characteristics to select optimal parsers:

\begin{itemize}
    \item \textbf{Native Extraction (70-80\% of pages):} For clean PDFs with text layers using PyMuPDF (0.1-0.3s/page, ~100MB RAM)
    \item \textbf{Layout-Aware Extraction (15-20\%):} For complex multi-column layouts using Marker with Surya layout detection (2-4s/page, ~600MB RAM)
    \item \textbf{OCR-Based Extraction (5-10\%):} Three-stage cascade (Tesseract $\rightarrow$ EasyOCR $\rightarrow$ PaddleOCR) for scanned documents (5-15s/page)
\end{itemize}

\textbf{Quality Validation:}

Each extraction is validated using coherence metrics:
\begin{equation}
\text{Coherence} = \frac{\text{Valid\_Characters}}{\text{Total\_Characters}} \times \frac{\text{Sentence\_Count}}{\text{Expected\_Sentences}}
\end{equation}

Extractions scoring below 0.5 are retried with stronger methods (maximum 3 attempts).

\textbf{Specialized Extractors:}

\begin{itemize}
    \item \textbf{Equations:} Multi-method LaTeX extraction (pattern matching, embedded LaTeX, optional Pix2Tex) with SymPy enrichment for variable extraction and natural language descriptions
    \item \textbf{Code Blocks:} Detection via markdown syntax, indentation patterns, and keyword density with language identification for 12+ languages
    \item \textbf{Tables:} Extraction with pdfplumber preserving structure, header detection, and cell alignment
    \item \textbf{Images:} Extraction with bounding boxes, caption detection using proximity heuristics, optional OCR for text-containing images
\end{itemize}

\subsubsection{Module 2: Processing}

The processing module transforms extracted content into semantically rich, type-classified blocks.

\textbf{Content Classification:}

Heuristic-based classification (1-3ms/block) assigns types without ML models:

\begin{itemize}
    \item \textbf{Math:} LaTeX delimiters, mathematical operators ($\int, \sum, \frac{}{}$)
    \item \textbf{Code:} Function definitions, imports, keywords (\texttt{def, class, function})
    \item \textbf{Table:} Pipe separators, column alignment patterns
    \item \textbf{List:} Bullet points, numbered items
    \item \textbf{Heading:} Short capitalized lines, section numbers, markdown headers
    \item \textbf{Text:} Default for standard prose
\end{itemize}

Each type receives a confidence score (0-1), with classification threshold at 0.6.

\textbf{Hierarchy Building:}

Document structure is detected via:
\begin{itemize}
    \item Markdown headers (\texttt{\#, \#\#, \#\#\#})
    \item Numbered sections (1., 1.1, 1.1.1)
    \item Chapter patterns (Chapter 1, CHAPTER I)
    \item Implicit capitalized headings
\end{itemize}

The resulting tree structure enables section-aware retrieval and hierarchical chunking.

\textbf{Relationship Mapping:}

Semantic links are established between blocks:
\begin{itemize}
    \item \textbf{Adjacent:} Sequential blocks (confidence: 1.0)
    \item \textbf{References:} Explicit mentions (Figure N, Table N, Equation N) (confidence: 0.8)
    \item \textbf{Explained\_by:} Math equations with nearby explanatory text (confidence: 0.7)
    \item \textbf{Described\_by:} Code blocks with surrounding descriptions (confidence: 0.7)
\end{itemize}

\textbf{Type-Specific Enrichment:}

\begin{itemize}
    \item \textbf{Math:} SymPy normalization, variable extraction, complexity indicators (calculus, algebra, matrices), natural language descriptions
    \item \textbf{Code:} Language detection, function/class extraction, import analysis, structure summary
    \item \textbf{Tables:} Row/column counting, header detection, type classification (comparison, financial, performance), summary generation
    \item \textbf{Text:} Definition extraction, acronym detection, key term identification (top 10 capitalized phrases), word/sentence counting
\end{itemize}

\textbf{Normalization:}

Text cleaning removes PDF artifacts:
\begin{itemize}
    \item Whitespace collapse and hyphenation rejoining
    \item Quote standardization (curly $\rightarrow$ straight)
    \item Header/footer removal (heuristic: short lines with "page"/numbers)
    \item Unit expansion (km $\rightarrow$ kilometers)
    \item Scientific notation conversion ($3.5 \times 10^2 \rightarrow 350.0$)
\end{itemize}

\subsubsection{Module 3: Chunking}

The chunking module creates optimized segments preserving semantic coherence and special content integrity.

\textbf{Fixed-Smart Chunking (Default):}

Our primary strategy targets 512 tokens (min 256, max 768) with 50-token overlap:

\begin{enumerate}
    \item Accumulate blocks until target size reached
    \item Check if next block exceeds maximum
    \item Finalize chunk at smart boundary (paragraph $>$ sentence $>$ clause)
    \item Add 50-token overlap from previous chunk
    \item Continue with remaining content
\end{enumerate}

\textbf{Boundary Detection:}

Content-aware boundary detector identifies safe split points:
\begin{itemize}
    \item Section breaks and paragraph boundaries (highest priority)
    \item Sentence endings (. ! ?)
    \item Clause boundaries (, ; :)
    \item Before/after special content blocks
\end{itemize}

\textbf{Never splits:}
\begin{itemize}
    \item Inside equations (\texttt{\$...\$, \$\$...\$\$})
    \item Inside code blocks (\texttt{```...```})
    \item Inside tables (\texttt{|...|})
    \item Mid-sentence unless forced by size constraints
\end{itemize}

\textbf{Alternative Strategies:}

\begin{itemize}
    \item \textbf{Semantic Chunking:} Embeds sentences, splits at $<0.7$ cosine similarity drops (100-200ms/page, requires embedding model)
    \item \textbf{Hierarchical Chunking:} Aligns chunks to sections/subsections from document hierarchy (20-40ms/page, requires clear structure)
\end{itemize}

\textbf{Chunk Enrichment:}

Each chunk receives metadata:
\begin{itemize}
    \item \textbf{Adjacent IDs:} Links to previous/next chunks for context expansion
    \item \textbf{Key Terms:} Pattern-matched capitalized phrases (top 10)
    \item \textbf{Summary:} First sentence or first 150 characters
    \item \textbf{Section Path:} Hierarchical path (Chapter $>$ Section $>$ Subsection)
    \item \textbf{Content Distribution:} Percentage breakdown by type (text, math, code, table)
\end{itemize}

\textbf{Validation:}

All chunks pass quality checks:
\begin{itemize}
    \item Size bounds (256-768 tokens)
    \item Complete sentences (no mid-sentence cuts)
    \item Balanced delimiters (matched brackets, quotes)
    \item Semantic coherence (not overly fragmented)
\end{itemize}

Invalid chunks are automatically corrected through merging (too small) or splitting at better boundaries (too large).

\subsubsection{Module 4: Embedding}

The embedding module generates dual representations: dense semantic vectors and sparse keyword vectors.

\textbf{Dense Embedding (BGE-small-en-v1.5):}

Neural embeddings capture semantic similarity:

\begin{lstlisting}[language=Python, caption={Dense embedding generation}]
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('BAAI/bge-small-en-v1.5', device='cuda')

# Apply content-type prefixes
prefixed_text = f"{content_prefix}: {chunk_content}"

# Generate and normalize embeddings
embeddings = model.encode(
    prefixed_text,
    batch_size=64,  # GPU-optimized
    normalize_embeddings=True  # L2 normalization
)
\end{lstlisting}

\textbf{Batching Strategy:}
\begin{itemize}
    \item GPU: batch\_size=64 (optimal throughput)
    \item CPU: batch\_size=16 (memory-constrained)
    \item Dynamic adjustment on OOM errors
    \item Length-based sorting for efficient padding
\end{itemize}

\textbf{Sparse Embedding (BM25):}

Statistical keyword matching using Okapi BM25 \cite{robertson1995okapi}:

\begin{equation}
\text{score}(D,Q) = \sum_{i=1}^{n} \text{IDF}(q_i) \cdot \frac{f(q_i,D) \cdot (k_1+1)}{f(q_i,D) + k_1 \cdot (1-b+b \cdot \frac{|D|}{\text{avgdl}})}
\end{equation}

where $f(q_i,D)$ is term frequency, $|D|$ is document length, avgdl is average document length, $k_1=1.5$ (term frequency saturation), and $b=0.75$ (length normalization).

\textbf{Tokenization:}
\begin{itemize}
    \item Lowercase conversion
    \item Regex-based word splitting
    \item Stopword removal (English stopwords)
    \item Minimum token length: 3 characters
\end{itemize}

\textbf{Two-Tier Caching:}

To avoid redundant computation:

\begin{itemize}
    \item \textbf{L1 Memory Cache:} LRU cache (10,000 embeddings, <1ms access)
    \item \textbf{L2 Disk Cache:} SQLite database (unlimited capacity, 5-10ms access)
    \item \textbf{Cache Key:} SHA-256(model\_name + content)
    \item \textbf{Hit Rate:} 60-80\% for similar documents
    \item \textbf{Cleanup:} Automatic removal of entries >30 days old
\end{itemize}

\textbf{Fallback Mechanism:}

If BGE model fails:
\begin{enumerate}
    \item Attempt CPU fallback
    \item If still fails, use TF-IDF vectorization (384 dimensions via feature selection)
    \item Lower quality but ensures system never fails
\end{enumerate}

\subsubsection{Module 5: Indexing}

The indexing module builds multi-store architecture for fast, accurate retrieval.

\textbf{Vector Store (FAISS IndexHNSWFlat):}

Hierarchical Navigable Small World (HNSW) index \cite{malkov2018hnsw} for approximate nearest neighbor search:

\begin{itemize}
    \item \textbf{Index Type:} IndexHNSWFlat (no quantization, maximum accuracy)
    \item \textbf{M:} 32 connections per layer (memory vs. accuracy trade-off)
    \item \textbf{efConstruction:} 40 (build-time accuracy)
    \item \textbf{efSearch:} 16 (query-time accuracy, tunable)
    \item \textbf{Distance Metric:} Cosine similarity (via dot product on normalized vectors)
    \item \textbf{Build Time:} ~30 seconds for 100k vectors
    \item \textbf{Query Time:} <50ms for k=10 in 100k vectors
    \item \textbf{Memory:} ~2KB per vector (384-dim)
\end{itemize}

\textbf{Keyword Store (BM25):}

Pure statistical ranking:
\begin{itemize}
    \item \textbf{Implementation:} rank-bm25 library
    \item \textbf{Index Build:} 1 second per 10k documents
    \item \textbf{Query Time:} <10ms regardless of corpus size
    \item \textbf{Memory:} ~2KB per document
    \item \textbf{Score Normalization:} Divided by max score for [0,1] range
\end{itemize}

\textbf{Metadata Store (SQLite + FTS5):}

Relational database with full-text search:

\begin{lstlisting}[language=SQL, caption={Metadata store schema}]
CREATE TABLE chunks (
    id INTEGER PRIMARY KEY,
    doc_id TEXT NOT NULL,
    content TEXT NOT NULL,
    content_type TEXT,  -- text/math/code/table
    page_num INTEGER,
    section_path TEXT,
    has_math BOOLEAN,
    has_code BOOLEAN,
    token_count INTEGER,
    metadata TEXT  -- JSON blob
);

CREATE VIRTUAL TABLE chunks_fts USING fts5(
    content,
    content=chunks,
    content_rowid=id
);

CREATE INDEX idx_chunks_type ON chunks(content_type);
\end{lstlisting}

\textbf{Graph Store (NetworkX):}

Relationship graph for context expansion:
\begin{itemize}
    \item \textbf{Nodes:} Chunks with metadata
    \item \textbf{Edges:} Directed relationships (adjacent, explains, references) with weights
    \item \textbf{Traversal:} BFS for 2-3 hop expansion
    \item \textbf{Storage:} Pickle serialization (~50MB for 100k chunks)
\end{itemize}

\textbf{Index Optimization:}

Automatic tuning for production deployment:
\begin{itemize}
    \item \textbf{FAISS efSearch Tuning:} Target 95\% recall with minimal latency
    \item \textbf{SQLite Optimization:} ANALYZE, VACUUM, PRAGMA optimize
    \item \textbf{Validation:} Test queries measure latency improvements
\end{itemize}

\subsubsection{Module 6: Retrieval}

The retrieval module implements hybrid search combining semantic and lexical matching.

\textbf{Hybrid Retrieval Strategy:}

Reciprocal Rank Fusion (RRF) \cite{cormack2009rrf} combines dense and sparse results:

\begin{equation}
\text{RRF\_score}(d) = \sum_{r \in \{dense, sparse\}} \frac{1}{k + rank_r(d)}
\end{equation}

where $k=60$ (default RRF constant) and $rank_r(d)$ is the rank of document $d$ in ranking $r$.

\textbf{Retrieval Pipeline:}

\begin{enumerate}
    \item \textbf{Dense Search:} Embed query, retrieve top-20 from FAISS (50ms)
    \item \textbf{Sparse Search:} Tokenize query, retrieve top-20 from BM25 (10ms)
    \item \textbf{RRF Fusion:} Combine rankings with RRF scoring (10ms)
    \item \textbf{Filtering:} Apply score threshold (min\_score=0.3), diversity limits (max 2/section)
    \item \textbf{Graph Expansion:} Add adjacent chunks if similarity $>0.7$ (optional, 50ms)
    \item \textbf{Return:} Top-k final results (default k=5)
\end{enumerate}

\textbf{Query Analysis:}

Heuristic-based query understanding without ML:
\begin{itemize}
    \item \textbf{Content Type:} Math/code/table indicators for type-specific retrieval
    \item \textbf{Intent:} Definition/procedure/example/comparison patterns
    \item \textbf{Complexity:} Simple ($<10$ words), moderate (10-20), complex ($>20$)
\end{itemize}

\textbf{Two-Level Caching:}

\begin{itemize}
    \item \textbf{L1:} In-memory LRU (100 queries, <1ms)
    \item \textbf{L2:} SQLite persistent (10k+ queries, <10ms)
    \item \textbf{Key:} hash(query + k + filters)
    \item \textbf{TTL:} 3600 seconds (1 hour)
    \item \textbf{Hit Rate:} 30-40\% typical usage
\end{itemize}

\textbf{Performance Characteristics:}

\begin{itemize}
    \item \textbf{Hybrid:} 120-270ms (embedding + search + fusion)
    \item \textbf{Semantic only:} 100ms
    \item \textbf{Keyword only:} 11ms
    \item \textbf{Recall@5:} 85\% (hybrid), 70\% (semantic), 65\% (keyword)
\end{itemize}

\subsubsection{Module 7: Generation}

The generation module synthesizes answers from retrieved context using quantized Mistral-7B.

\textbf{Prompt Construction:}

Type-specific prompts optimized for Mistral-7B-Instruct format:

\begin{lstlisting}[caption={Mistral prompt template}]
<s>[INST] {system_prompt}

Context:
{retrieved_chunks_with_citations}

Question: {user_query}

{type_specific_instructions}
[/INST]
\end{lstlisting}

\textbf{System Prompts by Type:}

\begin{itemize}
    \item \textbf{General:} "You are a helpful educational assistant. Answer based ONLY on provided context. Cite sources using [1], [2] format."
    \item \textbf{Math:} "You are a mathematics tutor. Show step-by-step work. Explain each step clearly. Use proper notation. Cite source of formulas."
    \item \textbf{Code:} "You are a programming expert. Explain before showing code. Use markdown code blocks. Highlight language-specific features."
    \item \textbf{Table:} "You are a data analyst. Present findings clearly. Analyze patterns. Use structured format. Cite specific table rows."
\end{itemize}

\textbf{Context Building:}

Token budget management for 4k context window:
\begin{itemize}
    \item Total tokens: 4000
    \item System/prompt: ~200
    \item Context: ~2500 (retrieved chunks)
    \item Query: ~100
    \item Generation space: ~1200
\end{itemize}

\textbf{Chunk Selection:}
\begin{enumerate}
    \item Prioritize by relevance score (highest first)
    \item Prioritize by content type if query-specific
    \item Truncate chunks exceeding max\_chunk\_tokens (512)
    \item Select until budget exhausted
\end{enumerate}

\textbf{Answer Validation:}

Five-dimensional quality assessment:

\begin{enumerate}
    \item \textbf{Groundedness (50-70\%):} Word overlap between answer and context, citation patterns
    \item \textbf{Relevance (20-30\%):} Query term coverage, intent matching, non-answer detection
    \item \textbf{Completeness (10-20\%):} Length vs. query complexity (optimal: 500-1000 words)
    \item \textbf{Quality (10-15\%):} Capitalization, sentence structure, vocabulary diversity
    \item \textbf{Citations (5-10\%):} Presence of [1], [2] markers, sufficient source references (min 1, ideal 3+)
\end{enumerate}

Validation threshold: score $\geq 0.6$ and issues $\leq 2$ for acceptance.

\textbf{Post-Processing:}

\begin{itemize}
    \item \textbf{Text Cleaning:} Remove generation artifacts ([INST], </s>), normalize whitespace
    \item \textbf{Equation Formatting:} Normalize LaTeX markers ($\backslash$[ $\backslash$] $\rightarrow$ \$\$ \$\$)
    \item \textbf{Code Formatting:} Infer language, add syntax highlighting
    \item \textbf{Citation Extraction:} Map [1], [2] to chunk IDs and source metadata
\end{itemize}

\textbf{Fallback Strategy:}

If generation fails or produces low-quality output:
\begin{enumerate}
    \item \textbf{Extractive Summary:} Concatenate top 3 chunks with citations
    \item \textbf{Simple Answer:} Return best chunk verbatim
    \item \textbf{Chunk List:} Format as numbered list for user browsing
\end{enumerate}

Fallback selection based on error type (timeout $\rightarrow$ extractive, OOM $\rightarrow$ simple).

\subsection{Multimodal Extension}

To enable visual document understanding, we integrate Florence-2 \cite{xiao2024florence2} vision encoder with our quantized LLM through a "Vision-as-Language" architecture that avoids expensive multimodal training.

\subsubsection{Vision Agent Architecture}

Instead of projecting image embeddings into LLM hidden space, we employ textual translation:

\begin{enumerate}
    \item \textbf{Visual Ingestion:} Convert document pages to images
    \item \textbf{Dense Captioning:} Florence-2 generates detailed textual descriptions using compound prompting:
    \begin{itemize}
        \item \texttt{<OD>} (Object Detection): Identify labeled diagram components
        \item \texttt{<MORE\_DETAILED\_CAPTION>}: Generate paragraph-level semantic descriptions
    \end{itemize}
    \item \textbf{Context Fusion:} Structure descriptions as "[Image Context]: The figure shows... [Vision Agent Output]"
    \item \textbf{Cross-Modal Reasoning:} Quantized LLM processes combined visual + textual context
\end{enumerate}

\subsubsection{Vision Model Selection}

We evaluated six lightweight vision encoders on 50 tri-modal QA pairs (Image, Text, Question) stratified by complexity:

\begin{itemize}
    \item \textbf{Text-Only (Type A):} Control questions ensuring vision doesn't degrade text performance
    \item \textbf{Vision-Only (Type B):} Questions requiring visual interpretation (e.g., "What color represents...")
    \item \textbf{Combined (Type C):} Synthesis questions requiring both modalities
\end{itemize}

\textbf{Models Evaluated:}
\begin{itemize}
    \item Microsoft Florence-2 (Base \& Large): Unified model trained on FLD-5B
    \item Moondream2 (1.86B): Edge-optimized VLM
    \item BLIP-Base: Baseline image captioning
    \item GIT-Base: Generative Image-to-Text transformer
    \item ViT-GPT2: Classic encoder-decoder baseline
\end{itemize}
