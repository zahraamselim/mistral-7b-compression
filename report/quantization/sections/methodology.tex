\section{Methodology}

\subsection{Base Model and Hardware}

This study utilizes \textbf{Mistral-7B-v0.1} \cite{jiang2023mistral} as the baseline model, selected for its parameter efficiency and strong performance profile. All experiments are conducted on a single \textbf{NVIDIA Tesla T4 GPU (16GB VRAM)} to simulate realistic edge deployment scenarios. The software environment includes PyTorch 2.x, Transformers 4.x, and quantization-specific libraries (bitsandbytes, AutoGPTQ, AutoAWQ, HQQ).

\subsection{Compression Techniques}

\subsubsection{NF4 (NormalFloat 4-bit)}

NF4 utilizes the quantile-based NormalFloat data type, which is information-theoretically optimal for zero-centered normal distributions. Double quantization is employed to further minimize memory footprint by quantizing the quantization constants themselves \cite{dettmers2023qlora}.

\textbf{Configuration:}
\begin{itemize}
\item Quantization type: NF4 (NormalFloat4)
\item Double quantization: Enabled
\item Compute dtype: float16
\item Group size: Per-tensor
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}
from transformers import BitsAndBytesConfig
import torch

nf4_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    quantization_config=nf4_config,
    device_map="auto"
)
\end{lstlisting}

\subsubsection{GPTQ}

GPTQ (Generative Pre-trained Transformer Quantization) is a post-training quantization method that quantizes weights to 2-8 bits while maintaining model quality through layer-wise optimization \cite{frantar2023gptq}.

\textbf{Method Overview:}

GPTQ addresses the computational challenges of optimal quantization by reformulating the problem as a series of layer-wise optimizations. For each layer's weight matrix $W \in \mathbb{R}^{d_{out} \times d_{in}}$, GPTQ solves:

\begin{equation}
\min_{\hat{W}} ||WX - \hat{W}X||_2^2
\end{equation}

where $W$ is the original weight matrix, $\hat{W}$ is the quantized weight matrix, and $X \in \mathbb{R}^{d_{in} \times n}$ represents calibration inputs with $n$ samples.

The key innovation is the use of approximate second-order information through the Hessian matrix $H = \frac{2}{n}XX^T$. GPTQ employs a greedy coordinate descent approach, quantizing weights column-by-column while adjusting subsequent weights to compensate for quantization error. For each weight $w_q$ being quantized:

\begin{equation}
\hat{w}_q = \text{quant}(w_q), \quad \delta_q = w_q - \hat{w}_q
\end{equation}

The quantization error $\delta_q$ is then propagated to remaining weights:

\begin{equation}
w_{q'} \leftarrow w_{q'} - \delta_q \cdot \frac{H_{qq'}}{H_{qq}} \quad \forall q' > q
\end{equation}

This Optimal Brain Quantization (OBQ)-inspired approach ensures that quantization errors are minimized across the entire weight matrix \cite{frantar2022optimal}.

\textbf{Group-wise Quantization:}

To balance precision and compression, GPTQ applies quantization in groups. Each group of $g$ weights (typically $g=128$) shares quantization parameters (scale $s$ and zero-point $z$):

\begin{equation}
\hat{w}_i = \text{round}\left(\frac{w_i}{s}\right) - z, \quad s = \frac{\max(w_g) - \min(w_g)}{2^b - 1}
\end{equation}

where $b$ is the number of bits (2, 3, or 4). Smaller group sizes increase accuracy at the cost of additional metadata storage.

\textbf{Quantization Process:}

We quantized Mistral-7B-v0.1 at three precision levels (4-bit, 3-bit, and 2-bit) using auto-gptq on a P100 GPU:

\begin{itemize}
\item Base model: mistralai/Mistral-7B-v0.1 (7.24B parameters)
\item Bits: 2, 3, 4
\item Group size: 128
\item Calibration dataset: WikiText-2 (128 samples, max length 512 tokens)
\item Damping factor: 0.01 (Hessian regularization)
\item Memory optimization: cache\_examples\_on\_gpu=False
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}[language=Python]
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from datasets import load_dataset

# Prepare calibration data
def prepare_calibration_data(tokenizer, n_samples=128, 
                            max_length=512):
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", 
                          split="train")
    dataset = dataset.filter(lambda x: len(x["text"]) > 200)
    
    examples = []
    for i in range(min(n_samples, len(dataset))):
        tokenized = tokenizer(dataset[i]["text"],
            return_tensors="pt", max_length=max_length,
            truncation=True, padding=False)
        examples.append({
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"]
        })
    return examples

calibration_data = prepare_calibration_data(tokenizer)

# Quantize model
model = AutoGPTQForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    quantize_config=BaseQuantizeConfig(
        bits=4, group_size=128, desc_act=False,
        damp_percent=0.01),
    max_memory={0: "10GiB", "cpu": "50GiB"})

model.quantize(calibration_data, use_triton=False, 
    batch_size=1, cache_examples_on_gpu=False)

model.save_quantized("./mistral-7b-gptq-4bit", 
                     use_safetensors=True)
\end{lstlisting}

The quantization process takes approximately 22 minutes per model on a P100 GPU, with memory usage optimized through CPU offloading.

\textbf{Optimized Inference with ExLlamaV2:}

For evaluation, we employed ExLlamaV2, an optimized inference engine for GPTQ models that provides 2-3x faster inference through specialized CUDA kernels:

\begin{lstlisting}[language=Python]
from exllamav2 import (ExLlamaV2, ExLlamaV2Config, 
                       ExLlamaV2Cache, ExLlamaV2Tokenizer)
from exllamav2.generator import (ExLlamaV2StreamingGenerator,
                                 ExLlamaV2Sampler)

# Load with ExLlamaV2
config = ExLlamaV2Config()
config.model_dir = "./mistral-7b-gptq-4bit"
config.prepare()
config.max_seq_len = 4096

model = ExLlamaV2(config)
cache = ExLlamaV2Cache(model, lazy=True)
model.load_autosplit(cache)
tokenizer = ExLlamaV2Tokenizer(config)

# Generate with optimized kernels
generator = ExLlamaV2StreamingGenerator(model, cache, 
                                        tokenizer)
settings = ExLlamaV2Sampler.Settings()
settings.temperature = 0.7
settings.top_p = 0.9

input_ids = tokenizer.encode(prompt)
generator.begin_stream(input_ids, settings)
\end{lstlisting}

ExLlamaV2 optimizations include custom CUDA kernels for 2/3/4-bit matrix multiplication, fused attention and MLP operations, and lazy KV cache loading to reduce memory overhead.

\textbf{Theoretical Compression:}

For Mistral-7B with 7.24B parameters, theoretical model sizes are:
\begin{itemize}
\item FP16 baseline: $7.24 \times 2 = 14.48$ GB
\item 4-bit GPTQ: $7.24 \times 0.5 + \text{overhead} \approx 3.62$ GB (4.0x compression)
\item 3-bit GPTQ: $7.24 \times 0.375 + \text{overhead} \approx 2.72$ GB (5.3x compression)
\item 2-bit GPTQ: $7.24 \times 0.25 + \text{overhead} \approx 1.81$ GB (8.0x compression)
\end{itemize}

where overhead includes quantization metadata (scales, zero-points) and unquantized components (embeddings, layer norms).

\subsubsection{AWQ (Activation-aware Weight Quantization)}

AWQ protects salient weights based on activation magnitudes, preserving the top 1\% most impactful weights with per-channel scaling \cite{lin2023awq}.

\textbf{Configuration:}
\begin{itemize}
\item Bits: 4
\item Group size: 128
\item Zero point: True
\item Version: GEMM
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_quantized(
    "TheBloke/Mistral-7B-Instruct-v0.1-AWQ",
    fuse_layers=True,
    safetensors=True
)
\end{lstlisting}

\subsubsection{HQQ (Half-Quadratic Quantization)}

HQQ performs fast, calibration-free quantization optimizing a custom loss function with on-the-fly parameter optimization \cite{badri2024hqq}.

\textbf{Configuration:}
\begin{itemize}
\item Bits: 4
\item Group size: 64
\item Axis: 1 (row-wise)
\item Compute dtype: float16
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}
from hqq.models.hf.base import AutoHQQHFModel
from hqq.core.quantize import BaseQuantizeConfig

quant_config = BaseQuantizeConfig(
    nbits=4, group_size=64, axis=1
)

model = AutoHQQHFModel.from_pretrained(
    "mistralai/Mistral-7B-Instruct-v0.1",
    torch_dtype=torch.float16
)
model.quantize_model(
    quant_config=quant_config, device='cuda'
)
\end{lstlisting}

\subsubsection{AutoRound}

AutoRound employs adaptive quantization with iterative optimization for per-layer parameter tuning.

\textbf{Configuration:}
\begin{itemize}
\item Bits: 4
\item Group size: 128
\item Samples: 64
\item Iterations: 50
\item Batch size: 4
\item Learning rate: 5e-3
\end{itemize}

\textbf{Implementation:}
\begin{lstlisting}
from auto_round import AutoRound

ar = AutoRound(
    model=MODEL_PATH,
    scheme="W4A16",
    bits=4,
    group_size=128,
    nsamples=64,
    iters=50,
    lr=5e-3,
    seqlen=1024,
    batch_size=4,
    amp_dtype=torch.float16
)

ar.quantize_and_save(
    output_dir=OUTPUT_DIR,
    format="auto_round"
)
\end{lstlisting}

\subsection{Evaluation Metrics}

We assess compressed models across two primary dimensions: computational efficiency and task performance.

\subsubsection{Efficiency Metrics}

\paragraph{Latency Measurements}
\begin{itemize}
\item \textbf{Average Latency:} Mean time per generated token (ms)
\item \textbf{Time-to-First-Token (TTFT):} Initial response latency
\item \textbf{Prefill vs. Decode:} Separate measurement of prompt processing vs. autoregressive generation
\end{itemize}

\paragraph{Throughput and Memory}
\begin{itemize}
\item \textbf{Throughput:} Tokens per second (tok/s)
\item \textbf{Peak Memory:} Maximum GPU memory (MB)
\item \textbf{Model Size:} Disk storage requirements (GB)
\end{itemize}

\paragraph{Computational Efficiency}
\begin{itemize}
\item \textbf{Model FLOPs Utilization (MFU):} Percentage of theoretical peak hardware FLOPs achieved:
\begin{equation}
\text{MFU} = \frac{\text{Achieved FLOPs/s}}{\text{Peak Hardware FLOPs/s}} \times 100\%
\end{equation}

\item \textbf{Energy Consumption:} Estimated energy per token (mJ):
\begin{equation}
E = (P_{\text{TDP}} - P_{\text{idle}}) \times t_{\text{inference}}
\end{equation}
where $P_{\text{idle}} \approx 0.3 \times P_{\text{TDP}}$
\end{itemize}

\subsubsection{Performance Benchmarks}

We evaluate models using established language modeling benchmarks:

\paragraph{Language Modeling}
\textbf{Perplexity:} Measured on WikiText-2 test set using sliding window evaluation (stride 512):
\begin{equation}
\text{PPL}(W) = \exp\left(-\frac{1}{N}\sum_{i=1}^{N}\ln P(w_i|w_{<i})\right)
\end{equation}

\paragraph{Core Task Benchmarks}
\begin{itemize}
\item \textbf{HellaSwag} (0-shot): Commonsense reasoning via sentence completion
\item \textbf{ARC-Easy} (0-shot): Grade-school science questions
\item \textbf{ARC-Challenge} (0-shot): Challenge-level scientific reasoning
\item \textbf{GSM8K} (8-shot): Grade-school math word problems
\item \textbf{MMLU} (5-shot): Multi-domain knowledge across 57 subjects
\item \textbf{HumanEval} (0-shot): Python code generation (pass@1)
\end{itemize}

All evaluations use the Language Model Evaluation Harness \cite{eval-harness} with consistent hyperparameters for reproducibility.
