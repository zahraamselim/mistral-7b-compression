@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de Las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, LÃ©lio and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{frantar2023gptq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  booktitle={Proceedings of the 40th International Conference on Machine Learning},
  pages={10017--10030},
  year={2023},
  organization={PMLR}
}

@inproceedings{frantar2022optimal,
  title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={35},
  pages={4475--4488},
  year={2022}
}

@software{exllamav2,
  title={ExLlamaV2: Optimized Inference Library for GPTQ Models},
  author={turboderp},
  year={2023},
  url={https://github.com/turboderp/exllamav2}
}

@software{autogptq,
  title={AutoGPTQ: An Easy-to-Use LLM Quantization Package with User-Friendly APIs},
  author={PanQiWei and others},
  year={2023},
  url={https://github.com/PanQiWei/AutoGPTQ}
}

@article{merity2016pointer,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016},
  note={WikiText dataset}
}

@inproceedings{lecun1990optimal,
  title={Optimal Brain Damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  booktitle={Advances in Neural Information Processing Systems},
  volume={2},
  pages={598--605},
  year={1990},
  note={Original optimal brain framework}
}

@article{nagel2021white,
  title={A White Paper on Neural Network Quantization},
  author={Nagel, Markus and Fournarakis, Marios and Amjad, Rana Ali and Bondarenko, Yelysei and van Baalen, Mart and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2106.08295},
  year={2021},
  note={Comprehensive quantization survey}
}

@inproceedings{gholami2022survey,
  title={A Survey of Quantization Methods for Efficient Neural Network Inference},
  author={Gholami, Amir and Kim, Sehoon and Dong, Zhen and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Low-Power Computer Vision},
  pages={291--326},
  year={2022},
  publisher={Chapman and Hall/CRC}
}

@article{dettmers2023case,
  title={The Case for 4-bit Precision: k-bit Inference Scaling Laws},
  author={Dettmers, Tim and Svirschevski, Ruslan and Egiazarian, Vage and Kuznedelev, Denis and Frantar, Elias and Ashkboos, Saleh and Borzunov, Alexander and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2212.09720},
  year={2023}
}

@inproceedings{park2022nuqmm,
  title={NUQMM: Quantized MatMul for Efficient Inference of Large-Scale Generative Language Models},
  author={Park, Gunho and Park, Baeseong and Kwon, Se Jung and Kim, Byeongwook and Lee, Youngjoo and Lee, Dongsoo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={8},
  pages={9552--9560},
  year={2023}
}

@article{lin2023awq,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Tang, Haotian and Yang, Shang and Dang, Xingyu and Han, Song},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023},
  note={Related quantization method}
}

@article{xiao2023smoothquant,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Ji and Seznec, Mickael and Wu, Hao and Demouth, Julien and Han, Song},
  journal={arXiv preprint arXiv:2211.10438},
  year={2023},
  note={Related quantization method}
}

@article{eval-harness,
  title={A Framework for Few-shot Language Model Evaluation},
  author={Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and others},
  journal={Version v0.4.0},
  year={2023},
  url={https://github.com/EleutherAI/lm-evaluation-harness},
  note={lm-eval-harness for task evaluation}
}

@article{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{sakaguchi2019winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={8732--8740},
  year={2020}
}

@inproceedings{bisk2020piqa,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{clark2018arc,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@misc{nvidia2023tensorrt,
  title={TensorRT: NVIDIA's High-Performance Deep Learning Inference SDK},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://developer.nvidia.com/tensorrt},
  note={Related optimization framework}
}

@article{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}

@inproceedings{sakaguchi2019winogrande,
  title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={8732--8740},
  year={2020}
}

@inproceedings{bisk2020piqa,
  title={PIQA: Reasoning about Physical Commonsense in Natural Language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  pages={7432--7439},
  year={2020}
}

@article{clark2018arc,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@misc{nvidia2023tensorrt,
  title={TensorRT: NVIDIA's High-Performance Deep Learning Inference SDK},
  author={{NVIDIA Corporation}},
  year={2023},
  url={https://developer.nvidia.com/tensorrt},
  note={Related optimization framework}
}