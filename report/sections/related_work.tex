\section{Related Work}
Model compression for large language models (LLMs) has emerged as a critical research area to address the substantial computational and memory requirements that hinder practical deployment. As LLMs scale to billions of parameters—such as GPT-175B requiring a minimum of 350GB of memory in FP16 format—the need for efficient compression techniques becomes increasingly urgent. Core compression paradigms include \textbf{quantization}, \textbf{pruning}, \textbf{knowledge distillation}, \textbf{low-rank factorization}, and \textbf{weight sharing}, each offering distinct trade-offs between model size, inference speed, and performance preservation \cite{zhu2023survey}.

\subsection{Quantization}
Quantization reduces the numerical precision of model parameters and activations, typically converting from 32-bit floating-point (FP32) or 16-bit floating-point (FP16) representations to lower-bit integer formats such as INT8 or INT4. This technique yields faster inference through reduced memory bandwidth requirements and smaller memory footprints, often with minimal accuracy degradation \cite{zhu2023survey}. Two fundamental strategies exist: \textit{quantization-aware training (QAT)} and \textit{post-training quantization (PTQ)}.

\paragraph{Quantization-Aware Training (QAT)}
QAT incorporates quantization simulation during training, enabling models to adapt to reduced precision. \textbf{LLM-QAT} \cite{abs-2305-17888} implements standard QAT through knowledge distillation from full-precision models. \textbf{BitDistiller} \cite{abs-2402-10631} advances QAT for sub-4-bit precisions through asymmetric quantization and adaptive clipping. \textbf{OneBit} \cite{abs-2402-11295} pushes boundaries with 1-bit parameter representation. However, QAT's limitation is substantial retraining cost, leading researchers to integrate Parameter-Efficient Fine-Tuning techniques like LoRA.

\paragraph{Post-Training Quantization (PTQ)}
PTQ applies quantization after training without retraining costs, making it attractive for resource-constrained practitioners. PTQ methods are categorized by their quantization targets:

\subparagraph{Weight-Only Quantization}
This approach compresses only model weights while maintaining full-precision activations. \textbf{LUT-GEMM} \cite{park2024lutgemm} uses binary-coding quantization for accelerated matrix multiplications. \textbf{GPTQ} \cite{frantar2023optq} proposes layer-wise quantization using inverse Hessian information for 3/4-bit quantization. \textbf{QuIP} \cite{chee2023quip} achieves 2-bit quantization through LDL decomposition of the Hessian matrix.

Several methods preserve sensitive weights: \textbf{AWQ} \cite{abs-2306-00978} stores the top 1\% most impactful weights in high-precision with per-channel scaling. \textbf{OWQ} \cite{LeeJKKP24} preserves weights sensitive to activation outliers. \textbf{SpQR} \cite{dettmers2024spqr} uses L2 error as a sensitivity metric. \textbf{SqueezeLLM} \cite{abs-2306-07629} introduces sensitivity-based weight clustering using k-means, achieving over 2x speedup.

\subparagraph{Weight-Activation Quantization}
This extends quantization to both weights and activations for true end-to-end low-precision inference. A key challenge is handling activation outliers. \textbf{ZeroQuant} \cite{YaoAZWLH22} pioneered this approach with group-wise weight quantization and token-wise activation quantization for INT8. \textbf{LLM.int8()} \cite{DettmersLBZ22} addresses outliers by storing outlier features in high-precision with vector-wise quantization for remaining features. \textbf{SmoothQuant} \cite{XiaoLSWDH23} uses per-channel scaling to smooth activation outliers. \textbf{RPTQ} \cite{abs-2304-01089} applies channel reordering to cluster activations. \textbf{OliVe} \cite{0003THL00LG023} proposes outlier-victim pair quantization. \textbf{OS+} \cite{wei-etal-2023-outlier} uses channel-wise shifting and scaling to handle outlier asymmetry. \textbf{LLM-FP4} \cite{liu-etal-2023-llm} explores floating-point formats (FP8/FP4) as alternatives. \textbf{OmniQuant} \cite{shao2024omniquant} shifts quantization challenges from activations to weights through clipping threshold optimization.

\subparagraph{KV Cache Quantization}
This targets key-value cache in attention mechanisms, which consumes substantial memory during autoregressive decoding. \textbf{KVQuant} \cite{abs-2401-18079} proposes Per-Channel Key Quantization, PreRoPE Key Quantization, and Non-Uniform KV cache quantization for 10M context length inference. \textbf{WKVQuant} \cite{abs-2402-12065} integrates past-only quantization with two-dimensional quantization and cross-block reconstruction regularization.

\subsection{Pruning}
Pruning reduces model size by removing redundant parameters, exploiting over-parameterization in large networks. Research shows up to 90\% of weights can be removed with minimal accuracy loss \cite{zhu2023survey}. Methods are categorized as unstructured, structured, or semi-structured.

\paragraph{Unstructured Pruning}
This removes individual weights without patterns, achieving high sparsity (50-99\%) but requiring specialized hardware support. \textbf{SparseGPT} \cite{FrantarA23} introduces one-shot pruning as sparse regression, achieving over 50\% sparsity on OPT-175B and BLOOM-176B without retraining. \textbf{Wanda} \cite{sun2024a} prunes weights with smallest magnitudes multiplied by input activation norms, eliminating retraining needs. \textbf{SAMSP} \cite{10445737} uses Hessian-based sensitivity metrics for dynamic sparsity allocation. \textbf{DSnoT} \cite{zhang2024dynamic} minimizes reconstruction error through iterative weight pruning-and-growing.

\paragraph{Structured Pruning}
This removes entire components (neurons, heads, layers) while preserving network structure, enabling hardware-agnostic acceleration. We categorize methods by pruning metrics:

\textit{Loss-based:} \textbf{LLM-Pruner} \cite{ma2023llmpruner} uses gradient information to identify dependent structures and select pruning groups (width reduction). \textbf{Shortened LLaMA} \cite{kim2024mefomo} performs one-shot depth pruning of Transformer blocks based on loss and second-order derivatives. Both use LoRA for rapid performance recovery.

\textit{Magnitude-based:} \textbf{FLAP} \cite{AnZYTW24} uses structured fluctuation metrics to identify prunable weight columns with adaptive structure search and baseline bias compensation. \textbf{SliceGPT} \cite{ashkboos2024slicegpt} leverages computational invariance and PCA to eliminate insignificant matrix columns/rows.

\textit{Regularization-based:} \textbf{Sheared LLaMA} uses Lagrange multipliers to impose constraints on pruned model shape, formulating pruning as constrained optimization with dynamic batch loading for efficient data utilization.

\paragraph{Semi-Structured Pruning}
This achieves fine-grained pruning with structural regularization through N:M sparsity patterns (N non-zero elements per M contiguous elements). \textbf{E-Sparse} \cite{abs-2310-15929} uses information entropy as an importance metric with global and local shuffling to optimize information distribution. SparseGPT and Wanda can be adapted to N:M patterns through block-wise weight partitioning.

\subsection{Knowledge Distillation}
Knowledge Distillation transfers knowledge from large teacher models to smaller student models. Methods are categorized as Black-box KD (only teacher outputs accessible) or White-box KD (teacher parameters/distributions available).

\paragraph{Black-box KD}
This prompts teacher LLMs to generate distillation datasets for student fine-tuning. Three primary approaches exist:

\textit{Chain-of-Thought Distillation:} \textbf{MT-COT} \cite{li2024explanations} uses multi-task learning with LLM-generated explanations. \textbf{SCOTT} \cite{WangWLGYR23} employs zero-shot CoT for diverse rationale generation. Decomposition-based methods distill problem decomposer and subproblem solver models. \textbf{PaD} uses Program-of-Thought rationales for mathematical reasoning. Interactive paradigms enable student feedback and self-reflection.

\textit{In-Context Learning Distillation:} \textbf{AICD} \cite{liu2024learning} performs meta-teacher forcing on in-context CoTs, jointly optimizing likelihood of all in-context CoTs. Meta In-context Tuning combines ICL objectives with language modeling objectives.

\textit{Instruction Following Distillation:} \textbf{Lion} \cite{jiang-etal-2023-lion} generates "hard" instructions for selective difficulty-based learning. \textbf{LaMini-LM} \cite{wu-etal-2024-lamini} develops 2.58M instructions for diverse model fine-tuning. \textbf{SELF-INSTRUCT} \cite{wang-etal-2023-self-instruct} uses student LMs as teachers to generate their own training data.

\paragraph{White-box KD}
This enables deeper understanding of teacher structure and representations. \textbf{MiniLLM} \cite{gu2024minillm} introduces reverse Kullback-Leibler divergence for generative LLM distillation. \textbf{GKD} \cite{agarwal2024generalized} trains students using self-generated outputs with teacher feedback. \textbf{TED} \cite{LiangZZHCZ23} proposes task-aware layer-wise distillation with task-aware filters for hidden representation alignment.

\subsection{Low-Rank Factorization}
This decomposes weight matrices $\mathbf{W} \in \mathbb{R}^{m \times n}$ into smaller components $\mathbf{W} \approx \mathbf{U}\mathbf{V}$, where $\mathbf{U} \in \mathbb{R}^{m \times k}$ and $\mathbf{V} \in \mathbb{R}^{k \times n}$ with $k \ll \min(m, n)$, reducing parameters from $m \times n$ to $(m + n) \times k$. \textbf{LPLR} \cite{SahaSP23} combines randomized low-rank factorization with low-precision quantization using random sketching. \textbf{ASVD} \cite{abs-2312-05821} scales weight matrices based on activation distributions and assigns adaptive layer-wise compression ratios by analyzing singular value distributions. \textbf{LASER} \cite{sharma2024the} selectively reduces rank of higher-order weight components, improving handling of rare training data and resistance to question paraphrasing.

\subsection{Weight Sharing}
This enforces parameter reuse across layers, reducing redundancy while maintaining capacity. \textbf{Basis Sharing} represents weight matrices as linear combinations of shared basis vectors with layer-specific coefficients, examining cross-layer basis sharing by compression error and grouping layers strategically. This approach surpasses SVD-based methods by up to 25\% perplexity reduction and 4\% accuracy improvement at 20-50\% compression ratios without fine-tuning.
