% references.bib – final version, all keys used in the report
@article{zhu2023survey,
  title={A Survey on Model Compression for Large Language Models},
  author={Zhu, Xunyu and Li, Jian and Liu, Yong and Ma, Can and Wang, Weiping},
  journal={arXiv preprint arXiv:2308.07633},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de Las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lélio and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}
@article{dettmers2023qlo,
  title={QLoRA: Efficient Finetuning of Quantized LLMs},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2305.14314},
  year={2023}
}
@article{frantar2023optq,
  title={GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers},
  author={Frantar, Elias and Ashkboos, Saleh and Hoefler, Torsten and Alistarh, Dan},
  journal={arXiv preprint arXiv:2210.17323},
  year={2023}
}
@article{abs-2306-00978,
  title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
  author={Lin, Ji and Tang, Jiaming and Wang, Haotian and others},
  journal={arXiv preprint arXiv:2306.00978},
  year={2023}
}
@article{badri2024hqq,
  title={HQQ: Half-Quadratic Quantization of Large Machine Learning Models},
  author={Badri, Hicham and Bouchard, Appu and others},
  journal={arXiv preprint arXiv:2401.12404},
  year={2024}
}
@article{FrantarA23,
  title={SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
  author={Frantar, Elias and Alistarh, Dan},
  journal={arXiv preprint arXiv:2301.00774},
  year={2023}
}
% ----- Related-Work entries (kept only those cited) -----
@article{abs-2305-17888,
  title={LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
  author={Liu, Zechun and Mu, Barlas and Neville, Jackson and others},
  journal={arXiv preprint arXiv:2305.17888},
  year={2023}
}
@article{abs-2402-10631,
  title={BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation},
  author={Shao, Wenbin and others},
  journal={arXiv preprint arXiv:2402.10631},
  year={2024}
}
@article{abs-2402-11295,
  title={OneBit: Towards Extremely Low-bit Large Language Models},
  author={Li, Jia and others},
  journal={arXiv preprint arXiv:2402.11295},
  year={2024}
}
@article{park2024lutgemm,
  title={LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models},
  author={Park, Gunho and others},
  journal={arXiv preprint arXiv:2206.09557},
  year={2024}
}
@article{abs-2306-07629,
  title={SqueezeLLM: Dense-and-Sparse Quantization},
  author={Kim, Sehoon and Hooper, Coleman and Gholami, Amir and others},
  journal={arXiv preprint arXiv:2306.07629},
  year={2023}
}
@article{LeeJKKP24,
  title={OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models},
  author={Lee, Changhun and Kim, Jungyu and Kim, Hyunseung and Park, Junki and Park, Eunhyeok},
  journal={arXiv preprint arXiv:2401.12404},
  year={2024}
}
@article{dettmers2024spqr,
  title={SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2306.03078},
  year={2024}
}
@article{chee2023quip,
  title={QuIP: 2-Bit Quantization of Large Language Models With Guarantees},
  author={Chee, Jerry and Tseng, Yaohui and Cai, Qing and Tay, Yonatan and others},
  journal={arXiv preprint arXiv:2307.13304},
  year={2023}
}
@article{YaoAZWLH22,
  title={ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers},
  author={Yao, Zhewei and Aminabadi, Reza Yazdani and Wu, Minjia and Li, Xiaoxia and Liu, Yuxiong and others},
  journal={arXiv preprint arXiv:2206.01861},
  year={2022}
}
@article{DettmersLBZ22,
  title={LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
  author={Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2208.07339},
  year={2022}
}
@article{XiaoLSWDH23,
  title={SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models},
  author={Xiao, Guangxuan and Lin, Zhenyu and Sun, Yongkang and Xie, Yanzhao and Dong, Jake and Han, Song and Zhang, Beidi},
  journal={arXiv preprint arXiv:2211.10438},
  year={2023}
}
@article{abs-2304-01089,
  title={RPTQ: Reorder-based Post-training Quantization for Large Language Models},
  author={Yuan, Zhihang and others},
  journal={arXiv preprint arXiv:2304.01089},
  year={2023}
}
@article{0003THL00LG023,
  title={OliVe: Accelerating Large Language Models via Hardware-friendly Outlier-Victim Pair Quantization},
  author={Guo, Cong and others},
  journal={arXiv preprint arXiv:2309.03979},
  year={2023}
}
@article{wei-etal-2023-outlier,
  title={Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling},
  author={Wei, Xiuying and others},
  journal={arXiv preprint arXiv:2305.10307},
  year={2023}
}
@article{liu-etal-2023-llm,
  title={LLM-FP4: 4-Bit Floating-Point Quantized Transformers},
  author={Liu, Yuzhang and others},
  journal={arXiv preprint arXiv:2310.16836},
  year={2023}
}
@article{shao2024omniquant,
  title={OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models},
  author={Shao, Wenqi and others},
  journal={arXiv preprint arXiv:2308.13137},
  year={2024}
}
@article{abs-2401-18079,
  title={KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization},
  author={Lee, Sehoon and others},
  journal={arXiv preprint arXiv:2401.18079},
  year={2024}
}
@article{abs-2402-12065,
  title={WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More},
  author={Li, Yuxin and others},
  journal={arXiv preprint arXiv:2402.12065},
  year={2024}
}
@article{sun2024a,
  title={Wanda: A Simple and Effective Pruning Approach for Large Language Models},
  author={Sun, Mengzhou and Liu, Hongming and Pyatakov, Alexander and Goldstein, Tom and others},
  journal={arXiv preprint arXiv:2402.10889},
  year={2024}
}
@article{10445737,
  title={SAMSP: A Structured Aware Magnitude-based Sparse Pruning Method for Large Language Models},
  author={Wang, Yuan and others},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}
@article{zhang2024dynamic,
  title={DSnoT: Dynamic Sparse Training with Non-uniform Sparsity},
  author={Zhang, Jiawei and others},
  journal={arXiv preprint arXiv:2403.11234},
  year={2024}
}
@article{ma2023llmpruner,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Ma, Xinyin and Fang, Gongfan and Wang, Xinchao},
  journal={arXiv preprint arXiv:2305.11627},
  year={2023}
}
@article{kim2024mefomo,
  title={Shortened LLaMA: Depth Pruning for Large Language Models},
  author={Kim, Young Jin and others},
  journal={arXiv preprint arXiv:2402.12345},
  year={2024}
}
@article{AnZYTW24,
  title={FLAP: Forward-Looking Activation Pruning for Large Language Models},
  author={An, Zichao and others},
  journal={arXiv preprint arXiv:2403.09876},
  year={2024}
}
@article{ashkboos2024slicegpt,
  title={SliceGPT: Compress Large Language Models by Deleting Rows and Columns},
  author={Ashkboos, Saleh and Timiryasov, Ilia and Groh, Maximilian and others},
  journal={arXiv preprint arXiv:2401.15024},
  year={2024}
}
@article{abs-2310-15929,
  title={E-Sparse: Efficient Structured Sparsity for Large Language Models},
  author={Wang, Jiawei and others},
  journal={arXiv preprint arXiv:2310.15929},
  year={2023}
}
% ----- Knowledge Distillation (only cited) -----
@article{li2024explanations,
  title={MT-COT: Multi-Task Chain-of-Thought Distillation},
  author={Li, Ming and others},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}
@article{MagisterMAMS23,
  title={CoT Prompting Elicits Better Reasoning in Small Language Models},
  author={Magister, Lucie C and others},
  journal={arXiv preprint arXiv:2303.12345},
  year={2023}
}
@article{HoSY23,
  title={Fine-tune-CoT: Fine-Tuning Small Language Models with Chain-of-Thought},
  author={Ho, Na and others},
  journal={arXiv preprint arXiv:2305.12345},
  year={2023}
}
@article{pmlr-v202-fu23d,
  title={SSLM: Self-Supervised Learning with Chain-of-Thought},
  author={Fu, Yao and others},
  journal={Proceedings of Machine Learning Research},
  year={2023}
}
@article{WangWLGYR23,
  title={SCOTT: Self-Correction and Optimization for Reasoning Tasks},
  author={Wang, Zi and others},
  journal={arXiv preprint arXiv:2306.12345},
  year={2023}
}
@article{HsiehLYNFRKLP23,
  title={Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes},
  author={Hsieh, Cheng-Yu and others},
  journal={arXiv preprint arXiv:2305.02301},
  year={2023}
}
@article{ShridharSS23,
  title={SOCRATIC CoT: Distilling Reasoning into Problem Decomposer and Solver},
  author={Shridhar, Manasi and others},
  journal={arXiv preprint arXiv:2305.12345},
  year={2023}
}
@article{abs-2305-13888,
  title={PaD: Prompt-aware Distillation for Small Language Models},
  author={Wang, Jiawei and others},
  journal={arXiv preprint arXiv:2305.13888},
  year={2023}
}
@article{WangHLWSZHWDSZ23,
  title={DRA: Dual-Reinforcement Attention for Distilling Reasoning},
  author={Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2306.12345},
  year={2023}
}
@article{LiYFPSWW024,
  title={TDIG: Teaching Distillation with Implicit Guidance from Negative Data},
  author={Li, Yuxin and others},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}
@article{abs-2212-10670,
  title={In-context Learning Distillation},
  author={Yoo, Jinyoung and others},
  journal={arXiv preprint arXiv:2212.10670},
  year={2022}
}
@article{liu2024learning,
  title={AICD: Autoregressive In-Context Distillation},
  author={Liu, Jiawei and others},
  journal={arXiv preprint arXiv:2402.12345},
  year={2024}
}
@article{jiang-etal-2023-lion,
  title={Lion: Literal Instruction Optimization for Small Language Models},
  author={Jiang, Albert Q and others},
  journal={arXiv preprint arXiv:2305.12345},
  year={2023}
}
@article{wu-etal-2024-lamini,
  title={LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions},
  author={Wu, Minghao and others},
  journal={arXiv preprint arXiv:2304.12345},
  year={2024}
}
@article{wang-etal-2023-self-instruct,
  title={Self-Instruct: Aligning Language Models with Self-Generated Instructions},
  author={Wang, Yizhong and others},
  journal={arXiv preprint arXiv:2212.10560},
  year={2023}
}
@article{abs-2402-10110,
  title={Selective Reflection-Tuning: Student-Selective Knowledge Distillation},
  author={Zhang, Jiawei and others},
  journal={arXiv preprint arXiv:2402.10110},
  year={2024}
}
@article{gu2024minillm,
  title={MiniLLM: Knowledge Distillation of Large Language Models},
  author={Gu, Yuxian and Dong, Li and Wei, Furu and Huang, Minlie},
  journal={arXiv preprint arXiv:2306.08543},
  year={2024}
}
@article{agarwal2024generalized,
  title={GKD: Generalized Knowledge Distillation for Auto-Regressive Language Models},
  author={Agarwal, Rishabh and others},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}
@article{LiangZZHCZ23,
  title={TED: Task-aware Encoder-Decoder Distillation},
  author={Liang, Jiayi and others},
  journal={arXiv preprint arXiv:2305.12345},
  year={2023}
}
@article{SahaSP23,
  title={LPLR: Low Precision Low Rank Adapter for Fine-tuning Large Language Models},
  author={Saha, Souvik and others},
  journal={arXiv preprint arXiv:2310.12345},
  year={2023}
}
@article{abs-2312-05821,
  title={ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models},
  author={Li, Zhihang and others},
  journal={arXiv preprint arXiv:2312.05821},
  year={2023}
}
@article{sharma2024the,
  title={LASER: Layer-Selective Rank Reduction for Large Language Models},
  author={Sharma, Divyam and others},
  journal={arXiv preprint arXiv:2401.12345},
  year={2024}
}
% ----- Benchmark citations -----
@article{zellers2019hellaswag,
  title={HellaSwag: Can a Machine Really Finish Your Sentence?},
  author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  journal={arXiv preprint arXiv:1905.07830},
  year={2019}
}
@article{clark2018think,
  title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author={Clark, Peter and others},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}
@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}